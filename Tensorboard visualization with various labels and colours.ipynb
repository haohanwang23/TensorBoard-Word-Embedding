{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haohanwang/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import bigrams \n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "from mlxtend.preprocessing import one_hot\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models import word2vec \n",
    "from gensim.models import doc2vec\n",
    "from gensim import utils\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function \n",
    "import argparse\n",
    "import sys\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "FLAG =None\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('tweets_with_emotion.csv',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9804,)\n"
     ]
    }
   ],
   "source": [
    "labels = data['Answer.sentiment']\n",
    "\n",
    "lb = np.asarray(labels)\n",
    "print(lb.shape)\n",
    "sentence = data['Contents']\n",
    "sentences = np.asarray(sentence)\n",
    "# print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9804,)\n"
     ]
    }
   ],
   "source": [
    "w = data['when']\n",
    "# w1 = pd.get_dummies(w)\n",
    "when = np.asarray(w)\n",
    "\n",
    "print(when.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.asarray(data['NumLabel'], dtype = np.int32)\n",
    "for i,val in enumerate(y):\n",
    "    y[i] = y[i] -1    \n",
    "y_ = one_hot(y, dtype='int')\n",
    "y_ = y_.astype(np.float32)\n",
    "# print(y_[:5])\n",
    "# print(y_.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "import gensim\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "\n",
    "def gensim_load_vec(path):\n",
    "    gensim_emb = gensim.models.Word2Vec.load_word2vec_format(path, binary=False)\n",
    "    vocab = gensim_emb.index2word\n",
    "    vec =gensim_emb.syn0\n",
    "    shape = gensim_emb.syn0.shape\n",
    "    return gensim_emb, vec, shape, vocab\n",
    "\n",
    "def map_word_frequency(document):\n",
    "    return Counter(itertools.chain(*document))\n",
    "\n",
    "def sentence2vec(tokenised_sentence_list, embedding_size, word_emb_model, a = 1e-3):\n",
    "    word_counts = map_word_frequency(tokenised_sentence_list)\n",
    "    sentence_set =[]\n",
    "    sentence_words = []\n",
    "    for sentence in tokenised_sentence_list:\n",
    "        vs = np.zeros(embedding_size)\n",
    "#         us = np.zeros(embedding_size)\n",
    "        sentence_length = len(sentence)\n",
    "        words =[]\n",
    "        for word in sentence:\n",
    "            a_value = a/(a+word_counts[word])\n",
    "            words.append(word)\n",
    "        try:\n",
    "            vs = np.add(vs, np.multiply(a_value, word_emb_model[word]))\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        vs = np.divide(vs, sentence_length)\n",
    "        \n",
    "        sentence_words.append([words[i:i+sentence_length] for i  in range(0, len(words), sentence_length)])\n",
    "        sentence_set.append(vs)\n",
    "        sent_words = np.squeeze(sentence_words,axis=0)\n",
    "#         sentence_words.append(us)\n",
    "        \n",
    "    #calculate PCA of this sentence set\n",
    "    pca = PCA(n_components= embedding_size)\n",
    "    pca.fit(np.array(sentence_set))\n",
    "    u = pca.explained_variance_ratio_\n",
    "    u = np.multiply(u, np.transpose(u))\n",
    "    \n",
    "    if len(u) < embedding_size:\n",
    "        for i in range(embedding_size -len(u)):\n",
    "            u = np.append(u, 0)\n",
    "            \n",
    "    sentence_vecs = []\n",
    "    \n",
    "    for vs in sentence_set:\n",
    "        sub = np.multiply(u,vs)\n",
    "        sentence_vecs.append(np.subtract(vs, sub))\n",
    "        sent_vecs = np.asarray(sentence_vecs)\n",
    "        \n",
    "    return sent_vecs,sent_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9804\n"
     ]
    }
   ],
   "source": [
    "# data = pd.read_csv('tweets_with_emotion.csv',encoding = 'utf-8')\n",
    "tweets =[]\n",
    "tweet= data['Contents'].tolist()\n",
    "for i in tweet:    \n",
    "    tweets.append(re.sub(r\"http://\\S+\" and r\"https://\\S+\", \"\", i))\n",
    "t = [list(x.split()) for x in tweets]\n",
    "print(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-06 14:26:06,974 : INFO : collecting all words and their counts\n",
      "2018-01-06 14:26:06,975 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-06 14:26:07,022 : INFO : collected 20983 word types from a corpus of 174318 raw words and 9804 sentences\n",
      "2018-01-06 14:26:07,023 : INFO : Loading a fresh vocabulary\n",
      "2018-01-06 14:26:07,070 : INFO : min_count=1 retains 20983 unique words (100% of original 20983, drops 0)\n",
      "2018-01-06 14:26:07,071 : INFO : min_count=1 leaves 174318 word corpus (100% of original 174318, drops 0)\n",
      "2018-01-06 14:26:07,135 : INFO : deleting the raw counts dictionary of 20983 items\n",
      "2018-01-06 14:26:07,137 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2018-01-06 14:26:07,138 : INFO : downsampling leaves estimated 140666 word corpus (80.7% of prior 174318)\n",
      "2018-01-06 14:26:07,139 : INFO : estimated required memory for 20983 words and 300 dimensions: 60850700 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-06 14:26:07,200 : INFO : resetting layer weights\n",
      "2018-01-06 14:26:07,541 : INFO : training model with 4 workers on 20983 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=4\n",
      "2018-01-06 14:26:08,555 : INFO : PROGRESS: at 60.76% examples, 424864 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-06 14:26:09,105 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-01-06 14:26:09,124 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-06 14:26:09,134 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-06 14:26:09,152 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-06 14:26:09,153 : INFO : training on 871590 raw words (702941 effective words) took 1.6s, 438153 effective words/s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging  \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality  \n",
    "min_word_count = 1   # Minimum word count  \n",
    "num_workers = 4       # Number of threads to run in parallel  \n",
    "context = 4          # Context window size  \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "from gensim.models import word2vec  \n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(t, workers=num_workers,  \n",
    "            size=num_features, sg=1, min_count = min_word_count,\n",
    "            window = context, sample = downsampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "sent_emb = sentence2vec(t, embedding_size, model)\n",
    "# print(sent_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_vec = sent_emb[0]\n",
    "emb_vec = np.asarray(emb_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.name_scope('input'):\n",
    "    X = tf.placeholder(tf.float32, [None, 300], name ='x-input')\n",
    "    Y = tf.placeholder(tf.float32, [None, 4],name = 'y-label')\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean',mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max_val', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min_val', tf.reduce_min(var))\n",
    "        tf.summary.histogram('var_histogram',var)\n",
    "\n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act = tf.nn.relu):\n",
    "    with tf.name_scope('Hidden_layer1'):\n",
    "\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_variable([output_dim])\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('output'):\n",
    "            preactive = tf.matmul(input_tensor, weights)+ biases\n",
    "\n",
    "        activations =act(preactive, name = 'output')\n",
    "        tf.summary.histogram('output', activations)\n",
    "        return activations\n",
    "\n",
    "hidden1 = nn_layer(X, 300, 100, 'hidden_layer1')\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder('float')\n",
    "    tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "    dropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "\n",
    "y = nn_layer(dropped, 100, 4,'layer2', act=tf.nn.softmax)\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    diff = tf.nn.softmax_cross_entropy_with_logits(logits = y, labels=Y)\n",
    "    cross_entropy = tf.reduce_mean(diff)\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "model.vocab = model.wv.vocab\n",
    "num_rows = len(sent_emb[1])\n",
    "dim = model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projector/emotion_embedding.ckpt-10000'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('train',\n",
    "                                    sess.graph)\n",
    "test_writer = tf.summary.FileWriter('test')\n",
    "w2v_10K = np.zeros((num_rows,dim))\n",
    "vec =[]\n",
    "with open(\"projector/projector.tsv\", 'w+') as file_metadata:\n",
    "    for i,word in enumerate(model.wv.index2word[:num_rows]):\n",
    "        w2v_10K[i] = model[word]\n",
    "        sentence = sentences[i]\n",
    "        text_word = TextBlob(word)\n",
    "        label = lb[i]\n",
    "        time = when[i]\n",
    "\n",
    "        if (i == 0):\n",
    "            file_metadata.write('word'+'\\t' +'when' +'\\t' + 'label'+'\\t'+'sentences'+'\\n')    \n",
    "        file_metadata.write(word + '\\t' + str(time)+'\\t' +str(label) +'\\t'+ str(sentence)  +'\\n')\n",
    "        \n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    embedding = tf.Variable(w2v_10K, trainable=False, name='emotions_embedding')    \n",
    "    \n",
    "tf.global_variables_initializer().run()\n",
    "saver = tf.train.Saver()\n",
    "writer = tf.summary.FileWriter('projector',sess.graph)\n",
    "config = projector.ProjectorConfig()\n",
    "embed = config.embeddings.add()\n",
    "embed.tensor_name = 'emotions_embedding'\n",
    "embed.metadata_path = os.path.join('projector/projector.tsv')\n",
    "projector.visualize_embeddings(writer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run(session = sess)\n",
    "saver = tf.train.Saver()\n",
    "config.model_checkpoint_path = os.path.join('projector','fashion.ckpt')\n",
    "saver.save(sess, 'projector/emotion_embedding.ckpt', global_step=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size, num_features = train_images.shape\n",
    "batch_size = 50\n",
    "epochs = 1\n",
    "num_iterations = size//batch_size\n",
    "test_step = 500\n",
    "\n",
    "for i in range(num_iterations*epochs):\n",
    "    offset = (i*batch_size)% size\n",
    "    batch_x = x_train[(offset):(offset+batch_size),:]\n",
    "    batch_y = y_train[offset:(offset+batch_size),:]\n",
    "    \n",
    "    \n",
    "    summary, _,loss = sess.run([merged, optimizer,cost],feed_dict={x: batch_x, y: batch_y})\n",
    "    train_writer.add_summary(summary,i)\n",
    "    if i % 100 == 0:\n",
    "        \n",
    "        print('Epoch {:<3} - Training Loss: {}'.format(i,loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
